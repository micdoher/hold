{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.io.sql as psql\n",
    "import psycopg2 as pg\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = pg.connect(\"host='192.168.1.8' dbname='holddb' user='pguser' password='ch@cky@b@s@bins'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = psql.read_sql(\"SELECT * FROM bookmark\", connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id    food_word_1    food_word_2    food_word_3  \\\n",
      "8      9          pasta                                 \n",
      "9     10          pasta                                 \n",
      "11    12          bread      olive oil         coffee   \n",
      "17    18          bread      olive oil         coffee   \n",
      "18    19          curry           rice         coffee   \n",
      "21    22      cous cous  tomatoe sauce     chick peas   \n",
      "22    23          bread         coffee      olive oil   \n",
      "27    28           eggs         coffee                  \n",
      "28    33           fish           pork       red wine   \n",
      "50    55          bread      olive oil         coffee   \n",
      "51    56           fish           rice         coffee   \n",
      "63    68        chicken       potatoes     white wine   \n",
      "64    69          bread         coffee                  \n",
      "67    72       tortilla           tuna  tomatoe sauce   \n",
      "70    75  vegtable soup     hard chese     soft chese   \n",
      "71    76       red wine                                 \n",
      "85    92     cereal bar         coffee                  \n",
      "86    93        chicken       red wine           beer   \n",
      "88    95          bread      olive oil         coffee   \n",
      "89    96          salad           beef         coffee   \n",
      "92    99        noodles        seafood                  \n",
      "117  125        gnocchi                                 \n",
      "118  126        peppers          pasta       red wine   \n",
      "128  136     croissante         coffee                  \n",
      "129  137  tomatoe sauce           eggs                  \n",
      "175  183          bread      olive oil         coffee   \n",
      "176  184      cous cous  vegtable soup    mixed salad   \n",
      "\n",
      "                          date  amount_1  amount_2  amount_3 weather     mood  \\\n",
      "8   2018-11-13 20:09:16.367520         2       NaN       NaN  cloudy      low   \n",
      "9   2018-11-13 20:09:16.377542         2       NaN       NaN  cloudy      low   \n",
      "11  2018-11-15 06:46:34.841999         2       1.0       1.0   sunny     high   \n",
      "17  2018-11-19 07:25:00.053350         2       2.0       1.0              low   \n",
      "18  2018-11-19 11:58:20.996075         3       2.0       1.0   sunny      low   \n",
      "21  2018-11-20 16:01:28.677631         2       2.0       2.0   sunny     wind   \n",
      "22  2018-11-21 07:07:44.416386         2       2.0       2.0   sunny     wind   \n",
      "27  2018-11-23 06:38:24.990883         3       2.0       NaN             high   \n",
      "28  2018-11-23 16:39:56.217593         2       2.0       2.0             wind   \n",
      "50  2018-12-11 07:58:06.125505         2       2.0       2.0             high   \n",
      "51  2018-12-12 13:32:17.943434         1       2.0       2.0             high   \n",
      "63  2019-01-08 20:53:17.590267         2       2.0       1.0            tired   \n",
      "64  2019-01-09 06:50:22.245822         2       2.0       NaN            tired   \n",
      "67  2019-01-09 14:32:31.693183         2       1.0       2.0            tired   \n",
      "70  2019-01-28 19:44:50.718649         2       2.0       1.0   sunny     high   \n",
      "71  2019-01-28 19:45:08.092399         1       NaN       NaN            tired   \n",
      "85  2019-02-08 08:45:45.142188         2       2.0       NaN  cloudy     high   \n",
      "86  2019-02-08 14:52:26.251422         1       2.0       1.0   sunny     high   \n",
      "88  2019-02-11 09:17:52.504903         3       2.0       2.0          neutral   \n",
      "89  2019-02-11 15:26:52.829675         3       1.0       1.0   sunny    tired   \n",
      "92  2019-02-12 19:53:03.241319         3       2.0       NaN             high   \n",
      "117 2019-03-20 08:41:48.274997         1       NaN       NaN          neutral   \n",
      "118 2019-03-20 20:25:52.284239         1       3.0       2.0   sunny     high   \n",
      "128 2019-04-08 07:57:20.670725         2       2.0       NaN            tired   \n",
      "129 2019-04-09 15:47:42.311455         2       2.0       NaN            tired   \n",
      "175 2020-01-14 09:00:17.149981         4       2.0       1.0  cloudy  neutral   \n",
      "176 2020-01-15 14:01:44.879782         2       2.0       1.0  cloudy  neutral   \n",
      "\n",
      "     user_id  \n",
      "8          1  \n",
      "9          1  \n",
      "11         1  \n",
      "17         1  \n",
      "18         1  \n",
      "21         1  \n",
      "22         1  \n",
      "27         1  \n",
      "28         1  \n",
      "50         1  \n",
      "51         1  \n",
      "63         1  \n",
      "64         1  \n",
      "67         1  \n",
      "70         1  \n",
      "71         1  \n",
      "85         1  \n",
      "86         1  \n",
      "88         1  \n",
      "89         1  \n",
      "92         1  \n",
      "117        1  \n",
      "118        1  \n",
      "128        1  \n",
      "129        1  \n",
      "175        1  \n",
      "176        1  \n"
     ]
    }
   ],
   "source": [
    "# 1. This is the 1st recommended approach with explanation \n",
    "#\"Create helper Series with compare by allergies. \n",
    "# Change order and use cumulative sum by Series.cumsum, \n",
    "# then it pass to GroupBy.cumcount and for second and third column compare by isin:\"\n",
    "\n",
    "s = df['mood'].eq('allergies').iloc[::-1].cumsum()\n",
    "df = df[df.groupby(s).cumcount(ascending=False).isin([1,2])]\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id    food_word_1    food_word_2    food_word_3  \\\n",
      "8      9          pasta                                 \n",
      "9     10          pasta                                 \n",
      "11    12          bread      olive oil         coffee   \n",
      "17    18          bread      olive oil         coffee   \n",
      "18    19          curry           rice         coffee   \n",
      "21    22      cous cous  tomatoe sauce     chick peas   \n",
      "22    23          bread         coffee      olive oil   \n",
      "27    28           eggs         coffee                  \n",
      "28    33           fish           pork       red wine   \n",
      "50    55          bread      olive oil         coffee   \n",
      "51    56           fish           rice         coffee   \n",
      "63    68        chicken       potatoes     white wine   \n",
      "64    69          bread         coffee                  \n",
      "67    72       tortilla           tuna  tomatoe sauce   \n",
      "70    75  vegtable soup     hard chese     soft chese   \n",
      "71    76       red wine                                 \n",
      "85    92     cereal bar         coffee                  \n",
      "86    93        chicken       red wine           beer   \n",
      "88    95          bread      olive oil         coffee   \n",
      "89    96          salad           beef         coffee   \n",
      "92    99        noodles        seafood                  \n",
      "117  125        gnocchi                                 \n",
      "118  126        peppers          pasta       red wine   \n",
      "128  136     croissante         coffee                  \n",
      "129  137  tomatoe sauce           eggs                  \n",
      "175  183          bread      olive oil         coffee   \n",
      "176  184      cous cous  vegtable soup    mixed salad   \n",
      "\n",
      "                          date  amount_1  amount_2  amount_3 weather     mood  \\\n",
      "8   2018-11-13 20:09:16.367520         2       NaN       NaN  cloudy      low   \n",
      "9   2018-11-13 20:09:16.377542         2       NaN       NaN  cloudy      low   \n",
      "11  2018-11-15 06:46:34.841999         2       1.0       1.0   sunny     high   \n",
      "17  2018-11-19 07:25:00.053350         2       2.0       1.0              low   \n",
      "18  2018-11-19 11:58:20.996075         3       2.0       1.0   sunny      low   \n",
      "21  2018-11-20 16:01:28.677631         2       2.0       2.0   sunny     wind   \n",
      "22  2018-11-21 07:07:44.416386         2       2.0       2.0   sunny     wind   \n",
      "27  2018-11-23 06:38:24.990883         3       2.0       NaN             high   \n",
      "28  2018-11-23 16:39:56.217593         2       2.0       2.0             wind   \n",
      "50  2018-12-11 07:58:06.125505         2       2.0       2.0             high   \n",
      "51  2018-12-12 13:32:17.943434         1       2.0       2.0             high   \n",
      "63  2019-01-08 20:53:17.590267         2       2.0       1.0            tired   \n",
      "64  2019-01-09 06:50:22.245822         2       2.0       NaN            tired   \n",
      "67  2019-01-09 14:32:31.693183         2       1.0       2.0            tired   \n",
      "70  2019-01-28 19:44:50.718649         2       2.0       1.0   sunny     high   \n",
      "71  2019-01-28 19:45:08.092399         1       NaN       NaN            tired   \n",
      "85  2019-02-08 08:45:45.142188         2       2.0       NaN  cloudy     high   \n",
      "86  2019-02-08 14:52:26.251422         1       2.0       1.0   sunny     high   \n",
      "88  2019-02-11 09:17:52.504903         3       2.0       2.0          neutral   \n",
      "89  2019-02-11 15:26:52.829675         3       1.0       1.0   sunny    tired   \n",
      "92  2019-02-12 19:53:03.241319         3       2.0       NaN             high   \n",
      "117 2019-03-20 08:41:48.274997         1       NaN       NaN          neutral   \n",
      "118 2019-03-20 20:25:52.284239         1       3.0       2.0   sunny     high   \n",
      "128 2019-04-08 07:57:20.670725         2       2.0       NaN            tired   \n",
      "129 2019-04-09 15:47:42.311455         2       2.0       NaN            tired   \n",
      "175 2020-01-14 09:00:17.149981         4       2.0       1.0  cloudy  neutral   \n",
      "176 2020-01-15 14:01:44.879782         2       2.0       1.0  cloudy  neutral   \n",
      "\n",
      "     user_id  \n",
      "8          1  \n",
      "9          1  \n",
      "11         1  \n",
      "17         1  \n",
      "18         1  \n",
      "21         1  \n",
      "22         1  \n",
      "27         1  \n",
      "28         1  \n",
      "50         1  \n",
      "51         1  \n",
      "63         1  \n",
      "64         1  \n",
      "67         1  \n",
      "70         1  \n",
      "71         1  \n",
      "85         1  \n",
      "86         1  \n",
      "88         1  \n",
      "89         1  \n",
      "92         1  \n",
      "117        1  \n",
      "118        1  \n",
      "128        1  \n",
      "129        1  \n",
      "175        1  \n",
      "176        1  \n"
     ]
    }
   ],
   "source": [
    "# 2. This is the 2nd recommended approach (without explanation) \n",
    "\n",
    "s = df['mood'].eq('allergies').iloc[::-1].cumsum().sort_index()\n",
    "df = df[(df.groupby(s).cumcount(ascending=False) < 3) & s.duplicated(keep='last')]\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8              pasta\n",
      "9              pasta\n",
      "10             bread\n",
      "11             bread\n",
      "12            coffee\n",
      "13              beef\n",
      "14             pasta\n",
      "15        cereal bar\n",
      "16          red wine\n",
      "17             bread\n",
      "18             curry\n",
      "19              nuts\n",
      "21         cous cous\n",
      "22             bread\n",
      "23              duck\n",
      "27              eggs\n",
      "28              fish\n",
      "29        cereal bar\n",
      "50             bread\n",
      "51              fish\n",
      "52             salad\n",
      "63           chicken\n",
      "64             bread\n",
      "65            fennel\n",
      "66           almonds\n",
      "67          tortilla\n",
      "68        cereal bar\n",
      "70     vegtable soup\n",
      "71          red wine\n",
      "72        cereal bar\n",
      "73        cereal bar\n",
      "85        cereal bar\n",
      "86           chicken\n",
      "87          pancakes\n",
      "88             bread\n",
      "89             salad\n",
      "90              eggs\n",
      "91              lamb\n",
      "92           noodles\n",
      "93             bread\n",
      "117          gnocchi\n",
      "118          peppers\n",
      "119             eggs\n",
      "128       croissante\n",
      "129    tomatoe sauce\n",
      "130            bread\n",
      "175            bread\n",
      "176        cous cous\n",
      "177           coffee\n",
      "Name: food_word_1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3. This is an attempt to add some date driven data into the mix \n",
    "\n",
    "s = df['mood'].eq('allergies').iloc[::-1].cumsum()\n",
    "df = df[df.groupby(s).cumcount(ascending=False).isin([0,1,2])]\n",
    "\n",
    "# BUT IS WITHIN A 48 HR PERIOD OF VALUES IN 'S'\n",
    "\n",
    "date_df = df # ['date'] < 48 s['date']\n",
    "print (date_df['food_word_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'fennel' in df.values:\n",
    "    print('Element exists in Dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
